{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Online 5g Machine-Learning with Nsight Compute\n",
    "\n",
    "## 06 Store vectors in shared memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can recognize that the data input vector of the algorithm is the same over the whole gaussian loop, and that the basis vectors are the same for every sample in the block. This means `we can share the basis (dictionary) vectors across different samples in the same block`. We assume a maximum size of the vectors and cache them in CUDA's low-latency `shared memory`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new version using shared memory is already available in the code. To enable the new version, we just need to set the `APSM_DETECT_VERSION` flag in line 73 of [apsm_versions.h](apsm/cpp/lib/apsm/apsm_versions.h)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "Open [apsm_detect.cu](apsm/cpp/lib/apsm/apsm_detect.cu) at line 615 to inspect the differences. You might notice that there is another intermediate step `APSM_DETECT_SPLIT` here. It's useful to do further analysis on the linear and gaussian sections of this kernel and understand better where more time is spent. It's not critical to progress in this lab, but you are free to come back here at the end and inspect it if you are interested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, after setting the `APSM_DETECT_VERSION` define to `apsm_version::APSM_DETECT_SHMEM`, re-compile the code with the following command and the collect a profiler report for this new version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /dli/task/ncu/apsm/cpp/build\n",
    "!make -j\n",
    "!ncu -k kernel_apsm_detect --set full --import-source yes -f -o /dli/task/ncu/report_shmem \\\n",
    "    bin/APSM_tool -m QAM16 -s ../data/offline/rx/time/rxData_QAM16_alltx_converted.bin -r ../data/offline/tx/NOMA_signals_qam16_complex.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps without instructor in `...`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "After collecting the results, switch to the Ubuntu instance with Nsight Compute and open the just created report file `/root/Desktop/reports/ncu/report_shmem.ncu-rep`. For easier comparison, `add the SPB step as a new baseline`. You can give each baseline a name by just hovering and typing over the existing one. (Since CG and SPB have very similar performance, you could also remove the older CG baseline to keep the comparison easier to read. Simply click on the colored box next to the respective baseline name)\n",
    "\n",
    "Before inspecting the actual performance, we can verify that the kernel is now in fact using shared memory in the `Launch Statistics` section. It shows that ~10KB of static shared memory are configured per CUDA block.\n",
    "\n",
    "<img src=\"images/ncu_report03_03.png\">\n",
    "\n",
    "Now, let's confirm which impact our changes had on the kernel runtime and metrics. Scroll back up on the Details page for the high-level comparison.\n",
    "\n",
    "<img src=\"images/ncu_report03_01.png\">\n",
    "\n",
    "As you can see, the new version is about `60% faster`.\n",
    "\n",
    "On the `Memory Workload Analysis` section, the `Memory Throughput` from DRAM has increased significantly. Given that the overall memory subsystem utilization `Memory Throughput` hasn't really changed across the three versions of the kernel, this indicates that we are using memory quite a bit more efficiently now.\n",
    "\n",
    "A primary contributor to this would be the `much reduced LG Throttle` stalls, as one can see on the `Warp State Statistics` section. We remember that this was the primary concern with the previous SPB implementation in step 5. This change reduced the pressure on the memory subsystem.\n",
    "\n",
    "<img src=\"images/ncu_report03_02.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, while the performance has improved, and the LG Throttle stalls are significantly reduced, we still see a stall reasons that causes relevant latency: `Stall Barrier`. Let's look at its description in the [documentation](https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#statistical-sampler):\n",
    "\n",
    "*Barrier*: Warp was stalled waiting for sibling warps at a CTA barrier. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a barrier. This causes some warps to wait a long time until other warps reach the synchronization point. Whenever possible, try to divide up the work into blocks of uniform workloads. Also, try to identify which barrier instruction causes the most stalls, and optimize the code executed before that synchronization point first.\n",
    "\n",
    "As we've seen in the Speed Of Light section on the top, Nsight Compute still suggests that the kernel is latency bound and that reducing these stalls could benefit kernel performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If short on time, you can move directly to the [summary](08_summary.ipynb)\n",
    "\n",
    "If you are interested in another step, continue optimizations in [step 07](07_balanced.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
